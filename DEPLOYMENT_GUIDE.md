# éƒ¨ç½²å’Œç»´æŠ¤æŒ‡å—

## ğŸ“‹ ç›®å½•
1. [éƒ¨ç½²æ¶æ„](#éƒ¨ç½²æ¶æ„)
2. [ç¯å¢ƒå‡†å¤‡](#ç¯å¢ƒå‡†å¤‡)
3. [å‰ç«¯éƒ¨ç½²](#å‰ç«¯éƒ¨ç½²)
4. [æ•°æ®åº“é…ç½®](#æ•°æ®åº“é…ç½®)
5. [CI/CDæµç¨‹](#CI/CDæµç¨‹)
6. [ç›‘æ§å’Œæ—¥å¿—](#ç›‘æ§å’Œæ—¥å¿—)
7. [å¤‡ä»½ç­–ç•¥](#å¤‡ä»½ç­–ç•¥)
8. [ç»´æŠ¤ä»»åŠ¡](#ç»´æŠ¤ä»»åŠ¡)
9. [æ•…éšœæ’é™¤](#æ•…éšœæ’é™¤)
10. [å®‰å…¨é…ç½®](#å®‰å…¨é…ç½®)

---

## éƒ¨ç½²æ¶æ„

### ğŸ—ï¸ æ•´ä½“æ¶æ„å›¾
```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚              ç”¨æˆ·è®¿é—®å±‚               â”‚
                    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
                    â”‚  â”‚   æ¡Œé¢ç«¯     â”‚      ç§»åŠ¨ç«¯         â”‚ â”‚
                    â”‚  â”‚ Desktop Web  â”‚   Mobile Web       â”‚ â”‚
                    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚              CDN/è´Ÿè½½å‡è¡¡               â”‚
                    â”‚        (Cloudflare/AWS CloudFront)     â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚              WebæœåŠ¡å™¨                  â”‚
                    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
                    â”‚  â”‚   Nginx     â”‚     é™æ€èµ„æº        â”‚  â”‚
                    â”‚  â”‚   åå‘ä»£ç†   â”‚     Static Files    â”‚  â”‚
                    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚            Supabase åç«¯               â”‚
                    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
                    â”‚  â”‚ PostgreSQL  â”‚   PostgREST API     â”‚  â”‚
                    â”‚  â”‚    æ•°æ®åº“    â”‚      AuthæœåŠ¡       â”‚  â”‚
                    â”‚  â”‚             â”‚   StorageæœåŠ¡       â”‚  â”‚
                    â”‚  â”‚             â”‚   RealtimeæœåŠ¡      â”‚  â”‚
                    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸŒ éƒ¨ç½²ç¯å¢ƒè§„åˆ’

#### 1. ç”Ÿäº§ç¯å¢ƒ (Production)
```yaml
ç¯å¢ƒåç§°: production
åŸŸå: logistics.company.com
æœåŠ¡å™¨é…ç½®:
  - CPU: 4æ ¸å¿ƒ
  - å†…å­˜: 8GB
  - å­˜å‚¨: 100GB SSD
  - å¸¦å®½: 100Mbps
æ•°æ®åº“: Supabase Pro Plan
CDN: Cloudflare Pro
SSLè¯ä¹¦: Let's Encrypt (è‡ªåŠ¨ç»­æœŸ)
ç›‘æ§: Uptime Robot + Sentry
```

#### 2. é¢„å‘å¸ƒç¯å¢ƒ (Staging)
```yaml
ç¯å¢ƒåç§°: staging
åŸŸå: staging-logistics.company.com
æœåŠ¡å™¨é…ç½®:
  - CPU: 2æ ¸å¿ƒ
  - å†…å­˜: 4GB
  - å­˜å‚¨: 50GB SSD
æ•°æ®åº“: Supabase Pro Plan (ç‹¬ç«‹å®ä¾‹)
ç”¨é€”: åŠŸèƒ½æµ‹è¯•ã€æ€§èƒ½æµ‹è¯•ã€ç”¨æˆ·éªŒæ”¶æµ‹è¯•
```

#### 3. å¼€å‘ç¯å¢ƒ (Development)
```yaml
ç¯å¢ƒåç§°: development
åŸŸå: dev-logistics.company.com
æœåŠ¡å™¨é…ç½®:
  - CPU: 2æ ¸å¿ƒ
  - å†…å­˜: 4GB
  - å­˜å‚¨: 30GB SSD
æ•°æ®åº“: Supabase Free Plan
ç”¨é€”: å¼€å‘æµ‹è¯•ã€åŠŸèƒ½éªŒè¯
```

---

## ç¯å¢ƒå‡†å¤‡

### ğŸ³ Dockerå®¹å™¨åŒ–éƒ¨ç½²

#### 1. Dockerfile
```dockerfile
# å¤šé˜¶æ®µæ„å»º
FROM node:18-alpine AS builder

# è®¾ç½®å·¥ä½œç›®å½•
WORKDIR /app

# å¤åˆ¶packageæ–‡ä»¶
COPY package*.json ./

# å®‰è£…ä¾èµ–
RUN npm ci --only=production && npm cache clean --force

# å¤åˆ¶æºä»£ç 
COPY . .

# æ„å»ºåº”ç”¨
ARG VITE_SUPABASE_URL
ARG VITE_SUPABASE_ANON_KEY
ENV VITE_SUPABASE_URL=$VITE_SUPABASE_URL
ENV VITE_SUPABASE_ANON_KEY=$VITE_SUPABASE_ANON_KEY

RUN npm run build

# ç”Ÿäº§é•œåƒ
FROM nginx:alpine AS production

# å¤åˆ¶æ„å»ºäº§ç‰©
COPY --from=builder /app/dist /usr/share/nginx/html

# å¤åˆ¶Nginxé…ç½®
COPY nginx.conf /etc/nginx/nginx.conf

# æš´éœ²ç«¯å£
EXPOSE 80

# å¯åŠ¨å‘½ä»¤
CMD ["nginx", "-g", "daemon off;"]
```

#### 2. docker-compose.yml
```yaml
version: '3.8'

services:
  # å‰ç«¯åº”ç”¨
  frontend:
    build:
      context: .
      target: production
      args:
        - VITE_SUPABASE_URL=${VITE_SUPABASE_URL}
        - VITE_SUPABASE_ANON_KEY=${VITE_SUPABASE_ANON_KEY}
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf:ro
      - ./ssl:/etc/ssl:ro
      - ./logs:/var/log/nginx
    environment:
      - NODE_ENV=production
    restart: unless-stopped
    networks:
      - logistics-network

  # Redisç¼“å­˜ (å¯é€‰)
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis-data:/data
    restart: unless-stopped
    networks:
      - logistics-network

volumes:
  redis-data:

networks:
  logistics-network:
    driver: bridge
```

#### 3. Nginxé…ç½®
```nginx
# nginx.conf
events {
    worker_connections 1024;
}

http {
    include /etc/nginx/mime.types;
    default_type application/octet-stream;

    # æ—¥å¿—æ ¼å¼
    log_format main '$remote_addr - $remote_user [$time_local] "$request" '
                    '$status $body_bytes_sent "$http_referer" '
                    '"$http_user_agent" "$http_x_forwarded_for"';

    access_log /var/log/nginx/access.log main;
    error_log /var/log/nginx/error.log warn;

    # åŸºç¡€é…ç½®
    sendfile on;
    tcp_nopush on;
    tcp_nodelay on;
    keepalive_timeout 65;
    types_hash_max_size 2048;
    client_max_body_size 50M;

    # Gzipå‹ç¼©
    gzip on;
    gzip_vary on;
    gzip_min_length 10240;
    gzip_proxied expired no-cache no-store private must-revalidate auth;
    gzip_types
        text/plain
        text/css
        text/xml
        text/javascript
        application/x-javascript
        application/javascript
        application/xml+rss
        application/json;

    # ä¸»æœåŠ¡å™¨é…ç½®
    server {
        listen 80;
        server_name logistics.company.com;
        
        # é‡å®šå‘åˆ°HTTPS
        return 301 https://$server_name$request_uri;
    }

    server {
        listen 443 ssl http2;
        server_name logistics.company.com;

        # SSLé…ç½®
        ssl_certificate /etc/ssl/logistics.company.com.crt;
        ssl_certificate_key /etc/ssl/logistics.company.com.key;
        ssl_protocols TLSv1.2 TLSv1.3;
        ssl_ciphers ECDHE-RSA-AES256-GCM-SHA512:DHE-RSA-AES256-GCM-SHA512;
        ssl_prefer_server_ciphers off;
        ssl_session_cache shared:SSL:10m;

        # å®‰å…¨å¤´
        add_header X-Frame-Options "SAMEORIGIN" always;
        add_header X-Content-Type-Options "nosniff" always;
        add_header X-XSS-Protection "1; mode=block" always;
        add_header Referrer-Policy "no-referrer-when-downgrade" always;
        add_header Content-Security-Policy "default-src 'self' https:; script-src 'self' 'unsafe-inline' 'unsafe-eval'; style-src 'self' 'unsafe-inline'; img-src 'self' data: https:;" always;

        # é™æ€èµ„æºæ ¹ç›®å½•
        root /usr/share/nginx/html;
        index index.html index.htm;

        # é™æ€èµ„æºç¼“å­˜
        location ~* \.(js|css|png|jpg|jpeg|gif|ico|svg|woff|woff2|ttf|eot)$ {
            expires 1y;
            add_header Cache-Control "public, immutable";
        }

        # APIä»£ç† (å¦‚æœéœ€è¦)
        location /api/ {
            proxy_pass https://your-project.supabase.co/;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
        }

        # SPAè·¯ç”±å¤„ç†
        location / {
            try_files $uri $uri/ /index.html;
        }

        # å¥åº·æ£€æŸ¥
        location /health {
            access_log off;
            return 200 "healthy\n";
            add_header Content-Type text/plain;
        }
    }
}
```

### â˜ï¸ äº‘æœåŠ¡å™¨é…ç½®

#### 1. Ubuntu 22.04 LTS åˆå§‹åŒ–
```bash
#!/bin/bash
# server-init.sh

# æ›´æ–°ç³»ç»Ÿ
sudo apt update && sudo apt upgrade -y

# å®‰è£…å¿…è¦è½¯ä»¶
sudo apt install -y curl wget git vim htop unzip

# å®‰è£…Docker
curl -fsSL https://get.docker.com -o get-docker.sh
sudo sh get-docker.sh
sudo usermod -aG docker $USER

# å®‰è£…Docker Compose
sudo curl -L "https://github.com/docker/compose/releases/download/v2.20.0/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose
sudo chmod +x /usr/local/bin/docker-compose

# å®‰è£…Node.js (å¯é€‰ï¼Œç”¨äºæœ¬åœ°æ„å»º)
curl -fsSL https://deb.nodesource.com/setup_18.x | sudo -E bash -
sudo apt install -y nodejs

# é…ç½®é˜²ç«å¢™
sudo ufw allow ssh
sudo ufw allow 80
sudo ufw allow 443
sudo ufw --force enable

# åˆ›å»ºåº”ç”¨ç›®å½•
sudo mkdir -p /opt/logistics-app
sudo chown $USER:$USER /opt/logistics-app

echo "æœåŠ¡å™¨åˆå§‹åŒ–å®Œæˆ!"
```

#### 2. ç³»ç»ŸæœåŠ¡é…ç½®
```bash
# åˆ›å»ºsystemdæœåŠ¡æ–‡ä»¶
sudo tee /etc/systemd/system/logistics-app.service > /dev/null <<EOF
[Unit]
Description=Logistics Management System
Requires=docker.service
After=docker.service

[Service]
Type=oneshot
RemainAfterExit=true
WorkingDirectory=/opt/logistics-app
ExecStart=/usr/local/bin/docker-compose up -d
ExecStop=/usr/local/bin/docker-compose down
TimeoutStartSec=0

[Install]
WantedBy=multi-user.target
EOF

# å¯ç”¨æœåŠ¡
sudo systemctl enable logistics-app.service
sudo systemctl start logistics-app.service
```

---

## å‰ç«¯éƒ¨ç½²

### ğŸš€ æ„å»ºå’Œéƒ¨ç½²æµç¨‹

#### 1. æœ¬åœ°æ„å»ºè„šæœ¬
```bash
#!/bin/bash
# build.sh

set -e

echo "å¼€å§‹æ„å»ºç‰©æµç®¡ç†ç³»ç»Ÿ..."

# æ£€æŸ¥ç¯å¢ƒå˜é‡
if [ -z "$VITE_SUPABASE_URL" ] || [ -z "$VITE_SUPABASE_ANON_KEY" ]; then
    echo "é”™è¯¯: ç¼ºå°‘å¿…è¦çš„ç¯å¢ƒå˜é‡"
    exit 1
fi

# æ¸…ç†æ—§çš„æ„å»ºæ–‡ä»¶
rm -rf dist/

# å®‰è£…ä¾èµ–
echo "å®‰è£…ä¾èµ–..."
npm ci

# è¿è¡Œæµ‹è¯• (å¯é€‰)
# npm run test

# ç±»å‹æ£€æŸ¥
echo "æ‰§è¡ŒTypeScriptæ£€æŸ¥..."
npm run type-check

# æ„å»ºåº”ç”¨
echo "æ„å»ºåº”ç”¨..."
npm run build

# å‹ç¼©æ„å»ºæ–‡ä»¶
echo "å‹ç¼©æ„å»ºæ–‡ä»¶..."
tar -czf logistics-app-$(date +%Y%m%d-%H%M%S).tar.gz dist/

echo "æ„å»ºå®Œæˆ!"
```

#### 2. éƒ¨ç½²è„šæœ¬
```bash
#!/bin/bash
# deploy.sh

set -e

SERVER_HOST="your-server-ip"
SERVER_USER="ubuntu"
APP_DIR="/opt/logistics-app"
BUILD_FILE="$1"

if [ -z "$BUILD_FILE" ]; then
    echo "ç”¨æ³•: $0 <build-file.tar.gz>"
    exit 1
fi

echo "å¼€å§‹éƒ¨ç½²åˆ°ç”Ÿäº§æœåŠ¡å™¨..."

# ä¸Šä¼ æ„å»ºæ–‡ä»¶
echo "ä¸Šä¼ æ„å»ºæ–‡ä»¶..."
scp $BUILD_FILE $SERVER_USER@$SERVER_HOST:/tmp/

# è¿œç¨‹éƒ¨ç½²
ssh $SERVER_USER@$SERVER_HOST << EOF
    set -e
    
    # å¤‡ä»½å½“å‰ç‰ˆæœ¬
    if [ -d "$APP_DIR/dist" ]; then
        sudo mv $APP_DIR/dist $APP_DIR/dist.backup.\$(date +%Y%m%d-%H%M%S)
    fi
    
    # è§£å‹æ–°ç‰ˆæœ¬
    cd /tmp
    tar -xzf $BUILD_FILE
    sudo mv dist $APP_DIR/
    sudo chown -R www-data:www-data $APP_DIR/dist
    
    # é‡å¯æœåŠ¡
    sudo systemctl reload nginx
    
    # æ¸…ç†
    rm -f /tmp/$BUILD_FILE
    
    echo "éƒ¨ç½²å®Œæˆ!"
EOF

echo "éƒ¨ç½²æˆåŠŸå®Œæˆ!"
```

#### 3. ç¯å¢ƒå˜é‡ç®¡ç†
```bash
# .env.production
VITE_SUPABASE_URL=https://your-project.supabase.co
VITE_SUPABASE_ANON_KEY=your-production-anon-key
VITE_APP_ENV=production
VITE_APP_VERSION=1.0.0
VITE_SENTRY_DSN=https://your-sentry-dsn
VITE_GOOGLE_ANALYTICS_ID=GA_MEASUREMENT_ID

# .env.staging
VITE_SUPABASE_URL=https://your-staging-project.supabase.co
VITE_SUPABASE_ANON_KEY=your-staging-anon-key
VITE_APP_ENV=staging
VITE_APP_VERSION=1.0.0-staging
```

### ğŸ“¦ é™æ€èµ„æºä¼˜åŒ–

#### 1. Viteæ„å»ºä¼˜åŒ–
```typescript
// vite.config.ts
import { defineConfig } from 'vite';
import react from '@vitejs/plugin-react';
import { resolve } from 'path';

export default defineConfig({
  plugins: [react()],
  resolve: {
    alias: {
      '@': resolve(__dirname, 'src'),
    },
  },
  build: {
    target: 'es2015',
    outDir: 'dist',
    assetsDir: 'assets',
    sourcemap: process.env.NODE_ENV !== 'production',
    minify: 'terser',
    terserOptions: {
      compress: {
        drop_console: process.env.NODE_ENV === 'production',
        drop_debugger: true,
      },
    },
    rollupOptions: {
      output: {
        manualChunks: {
          vendor: ['react', 'react-dom'],
          ui: ['@radix-ui/react-dialog', '@radix-ui/react-select'],
          charts: ['recharts'],
          utils: ['date-fns', 'clsx'],
        },
      },
    },
    chunkSizeWarningLimit: 1000,
  },
  server: {
    port: 3000,
    host: true,
    proxy: {
      '/api': {
        target: 'http://localhost:54321',
        changeOrigin: true,
      },
    },
  },
});
```

#### 2. èµ„æºå‹ç¼©å’Œç¼“å­˜
```bash
# æ„å»ºåä¼˜åŒ–è„šæœ¬
#!/bin/bash
# optimize-assets.sh

BUILD_DIR="dist"

echo "ä¼˜åŒ–é™æ€èµ„æº..."

# å‹ç¼©å›¾ç‰‡ (éœ€è¦å®‰è£…imagemin)
find $BUILD_DIR -name "*.png" -exec pngcrush -ow {} \;
find $BUILD_DIR -name "*.jpg" -exec jpegoptim --strip-all {} \;

# ç”Ÿæˆèµ„æºæ¸…å•
find $BUILD_DIR -type f -name "*.js" -o -name "*.css" -o -name "*.png" -o -name "*.jpg" | \
    sed "s|$BUILD_DIR/||" > $BUILD_DIR/assets-manifest.txt

# è®¾ç½®ç¼“å­˜å¤´æ–‡ä»¶
cat > $BUILD_DIR/.htaccess << EOF
# é™æ€èµ„æºç¼“å­˜
<FilesMatch "\.(css|js|png|jpg|jpeg|gif|ico|svg|woff|woff2|ttf|eot)$">
    ExpiresActive On
    ExpiresDefault "access plus 1 year"
    Header set Cache-Control "public, immutable"
</FilesMatch>

# HTMLæ–‡ä»¶ä¸ç¼“å­˜
<FilesMatch "\.html$">
    ExpiresActive On
    ExpiresDefault "access plus 0 seconds"
    Header set Cache-Control "no-cache, no-store, must-revalidate"
</FilesMatch>
EOF

echo "èµ„æºä¼˜åŒ–å®Œæˆ!"
```

---

## æ•°æ®åº“é…ç½®

### ğŸ—„ï¸ Supabaseé…ç½®

#### 1. é¡¹ç›®åˆå§‹åŒ–
```sql
-- åˆå§‹åŒ–è„šæœ¬ init.sql
-- åˆ›å»ºæ‰©å±•
CREATE EXTENSION IF NOT EXISTS "uuid-ossp";
CREATE EXTENSION IF NOT EXISTS "pg_stat_statements";

-- åˆ›å»ºè‡ªå®šä¹‰å‡½æ•°
CREATE OR REPLACE FUNCTION update_updated_at_column()
RETURNS TRIGGER AS $$
BEGIN
    NEW.updated_at = NOW();
    RETURN NEW;
END;
$$ language 'plpgsql';

-- åˆ›å»ºå®¡è®¡æ—¥å¿—è¡¨
CREATE TABLE IF NOT EXISTS audit_logs (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    table_name TEXT NOT NULL,
    record_id UUID,
    action TEXT NOT NULL,
    old_values JSONB,
    new_values JSONB,
    user_id UUID REFERENCES auth.users(id),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

-- å¯ç”¨RLS
ALTER TABLE audit_logs ENABLE ROW LEVEL SECURITY;

-- åˆ›å»ºå®¡è®¡æ—¥å¿—ç­–ç•¥
CREATE POLICY "audit_logs_select_policy" ON audit_logs
    FOR SELECT USING (
        EXISTS (
            SELECT 1 FROM user_profiles 
            WHERE id = auth.uid() AND role = 'admin'
        )
    );
```

#### 2. ç¯å¢ƒå˜é‡é…ç½®
```bash
# Supabaseç¯å¢ƒå˜é‡
export SUPABASE_URL="https://your-project.supabase.co"
export SUPABASE_ANON_KEY="your-anon-key"
export SUPABASE_SERVICE_ROLE_KEY="your-service-role-key"
export DATABASE_URL="postgresql://postgres:[password]@db.[ref].supabase.co:5432/postgres"

# æ•°æ®åº“è¿æ¥æ± é…ç½®
export DB_POOL_MIN=2
export DB_POOL_MAX=10
export DB_POOL_IDLE_TIMEOUT=30000
```

#### 3. æ•°æ®åº“è¿ç§»è„šæœ¬
```bash
#!/bin/bash
# migrate.sh

set -e

echo "æ‰§è¡Œæ•°æ®åº“è¿ç§»..."

# æ£€æŸ¥Supabase CLI
if ! command -v supabase &> /dev/null; then
    echo "é”™è¯¯: è¯·å…ˆå®‰è£…Supabase CLI"
    exit 1
fi

# ç™»å½•Supabase
supabase login

# é“¾æ¥é¡¹ç›®
supabase link --project-ref your-project-ref

# ç”Ÿæˆç±»å‹æ–‡ä»¶
supabase gen types typescript --local > src/types/database.ts

# æ¨é€è¿ç§»
supabase db push

# é‡ç½®æ•°æ®åº“ (ä»…å¼€å‘ç¯å¢ƒ)
if [ "$NODE_ENV" = "development" ]; then
    supabase db reset
fi

echo "æ•°æ®åº“è¿ç§»å®Œæˆ!"
```

### ğŸ”„ æ•°æ®å¤‡ä»½å’Œæ¢å¤

#### 1. è‡ªåŠ¨å¤‡ä»½è„šæœ¬
```bash
#!/bin/bash
# backup.sh

set -e

BACKUP_DIR="/opt/backups/logistics"
DATE=$(date +%Y%m%d_%H%M%S)
RETENTION_DAYS=30

# åˆ›å»ºå¤‡ä»½ç›®å½•
mkdir -p $BACKUP_DIR

echo "å¼€å§‹æ•°æ®åº“å¤‡ä»½..."

# å¤‡ä»½æ•°æ®åº“
pg_dump $DATABASE_URL > $BACKUP_DIR/logistics_backup_$DATE.sql

# å‹ç¼©å¤‡ä»½æ–‡ä»¶
gzip $BACKUP_DIR/logistics_backup_$DATE.sql

# å¤‡ä»½æ–‡ä»¶ä¸Šä¼ åˆ°äº‘å­˜å‚¨ (å¯é€‰)
# aws s3 cp $BACKUP_DIR/logistics_backup_$DATE.sql.gz s3://your-backup-bucket/

# æ¸…ç†æ—§å¤‡ä»½
find $BACKUP_DIR -name "logistics_backup_*.sql.gz" -mtime +$RETENTION_DAYS -delete

echo "å¤‡ä»½å®Œæˆ: logistics_backup_$DATE.sql.gz"
```

#### 2. æ•°æ®æ¢å¤è„šæœ¬
```bash
#!/bin/bash
# restore.sh

BACKUP_FILE="$1"

if [ -z "$BACKUP_FILE" ]; then
    echo "ç”¨æ³•: $0 <backup-file.sql.gz>"
    exit 1
fi

echo "è­¦å‘Š: æ­¤æ“ä½œå°†è¦†ç›–ç°æœ‰æ•°æ®åº“!"
read -p "ç¡®è®¤ç»§ç»­? (y/N): " confirm

if [ "$confirm" != "y" ]; then
    echo "æ“ä½œå·²å–æ¶ˆ"
    exit 0
fi

echo "å¼€å§‹æ¢å¤æ•°æ®åº“..."

# è§£å‹å¤‡ä»½æ–‡ä»¶
gunzip -c $BACKUP_FILE > /tmp/restore.sql

# æ¢å¤æ•°æ®åº“
psql $DATABASE_URL < /tmp/restore.sql

# æ¸…ç†ä¸´æ—¶æ–‡ä»¶
rm -f /tmp/restore.sql

echo "æ•°æ®åº“æ¢å¤å®Œæˆ!"
```

---

## CI/CDæµç¨‹

### ğŸ”„ GitHub Actionsé…ç½®

#### 1. ä¸»å·¥ä½œæµ
```yaml
# .github/workflows/deploy.yml
name: Deploy to Production

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]

env:
  NODE_VERSION: '18'

jobs:
  test:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
        
      - name: Setup Node.js
        uses: actions/setup-node@v3
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
          
      - name: Install dependencies
        run: npm ci
        
      - name: Run type check
        run: npm run type-check
        
      - name: Run linter
        run: npm run lint
        
      - name: Run tests
        run: npm run test:ci
        
  build:
    needs: test
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
        
      - name: Setup Node.js
        uses: actions/setup-node@v3
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
          
      - name: Install dependencies
        run: npm ci
        
      - name: Build application
        run: npm run build
        env:
          VITE_SUPABASE_URL: ${{ secrets.VITE_SUPABASE_URL }}
          VITE_SUPABASE_ANON_KEY: ${{ secrets.VITE_SUPABASE_ANON_KEY }}
          
      - name: Upload build artifacts
        uses: actions/upload-artifact@v3
        with:
          name: dist
          path: dist/
          
  deploy:
    needs: build
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    
    steps:
      - name: Download build artifacts
        uses: actions/download-artifact@v3
        with:
          name: dist
          path: dist/
          
      - name: Deploy to server
        uses: appleboy/ssh-action@v0.1.5
        with:
          host: ${{ secrets.SERVER_HOST }}
          username: ${{ secrets.SERVER_USER }}
          key: ${{ secrets.SERVER_SSH_KEY }}
          script: |
            # å¤‡ä»½å½“å‰ç‰ˆæœ¬
            sudo mv /opt/logistics-app/dist /opt/logistics-app/dist.backup.$(date +%Y%m%d-%H%M%S) || true
            
            # éƒ¨ç½²æ–°ç‰ˆæœ¬
            sudo mkdir -p /opt/logistics-app/dist
            
            # è¿™é‡Œéœ€è¦å®é™…çš„æ–‡ä»¶ä¼ è¾“é€»è¾‘
            # å¯ä»¥ä½¿ç”¨rsyncæˆ–å…¶ä»–æ–¹å¼
            
            # é‡å¯æœåŠ¡
            sudo systemctl reload nginx
            
            echo "éƒ¨ç½²å®Œæˆ!"
```

#### 2. é¢„å‘å¸ƒç¯å¢ƒå·¥ä½œæµ
```yaml
# .github/workflows/staging.yml
name: Deploy to Staging

on:
  push:
    branches: [develop]

jobs:
  deploy-staging:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
        
      - name: Setup Node.js
        uses: actions/setup-node@v3
        with:
          node-version: '18'
          cache: 'npm'
          
      - name: Install dependencies
        run: npm ci
        
      - name: Build for staging
        run: npm run build
        env:
          VITE_SUPABASE_URL: ${{ secrets.STAGING_SUPABASE_URL }}
          VITE_SUPABASE_ANON_KEY: ${{ secrets.STAGING_SUPABASE_ANON_KEY }}
          VITE_APP_ENV: staging
          
      - name: Deploy to staging server
        # éƒ¨ç½²åˆ°é¢„å‘å¸ƒç¯å¢ƒçš„é€»è¾‘
        run: echo "Deploy to staging"
```

#### 3. è´¨é‡æ£€æŸ¥å·¥ä½œæµ
```yaml
# .github/workflows/quality.yml
name: Code Quality Check

on:
  pull_request:
    branches: [main, develop]

jobs:
  quality-check:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
        
      - name: Setup Node.js
        uses: actions/setup-node@v3
        with:
          node-version: '18'
          cache: 'npm'
          
      - name: Install dependencies
        run: npm ci
        
      - name: Run ESLint
        run: npm run lint -- --format=json --output-file=eslint-report.json
        
      - name: Run Prettier check
        run: npm run format:check
        
      - name: Run type check
        run: npm run type-check
        
      - name: Run tests with coverage
        run: npm run test:coverage
        
      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v3
        with:
          file: ./coverage/lcov.info
          
      - name: Comment PR
        uses: actions/github-script@v6
        if: github.event_name == 'pull_request'
        with:
          script: |
            const fs = require('fs');
            const eslintReport = JSON.parse(fs.readFileSync('eslint-report.json', 'utf8'));
            const errorCount = eslintReport.reduce((sum, file) => sum + file.errorCount, 0);
            const warningCount = eslintReport.reduce((sum, file) => sum + file.warningCount, 0);
            
            const comment = `
            ## ä»£ç è´¨é‡æ£€æŸ¥ç»“æœ
            
            - ESLint é”™è¯¯: ${errorCount}
            - ESLint è­¦å‘Š: ${warningCount}
            - TypeScript æ£€æŸ¥: âœ… é€šè¿‡
            - æµ‹è¯•è¦†ç›–ç‡: æŸ¥çœ‹è¯¦ç»†æŠ¥å‘Š
            `;
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });
```

### ğŸš€ éƒ¨ç½²ç­–ç•¥

#### 1. è“ç»¿éƒ¨ç½²
```bash
#!/bin/bash
# blue-green-deploy.sh

set -e

BLUE_DIR="/opt/logistics-app/blue"
GREEN_DIR="/opt/logistics-app/green"
CURRENT_LINK="/opt/logistics-app/current"
NEW_BUILD="$1"

if [ -z "$NEW_BUILD" ]; then
    echo "ç”¨æ³•: $0 <new-build.tar.gz>"
    exit 1
fi

# ç¡®å®šå½“å‰ç¯å¢ƒ
if [ -L "$CURRENT_LINK" ]; then
    CURRENT_ENV=$(readlink $CURRENT_LINK | grep -o 'blue\|green')
    if [ "$CURRENT_ENV" = "blue" ]; then
        NEW_ENV="green"
        NEW_DIR="$GREEN_DIR"
    else
        NEW_ENV="blue"
        NEW_DIR="$BLUE_DIR"
    fi
else
    NEW_ENV="blue"
    NEW_DIR="$BLUE_DIR"
fi

echo "éƒ¨ç½²åˆ° $NEW_ENV ç¯å¢ƒ..."

# æ¸…ç†ç›®æ ‡ç›®å½•
sudo rm -rf $NEW_DIR
sudo mkdir -p $NEW_DIR

# è§£å‹æ–°ç‰ˆæœ¬
tar -xzf $NEW_BUILD -C $NEW_DIR

# å¥åº·æ£€æŸ¥
echo "æ‰§è¡Œå¥åº·æ£€æŸ¥..."
# è¿™é‡Œå¯ä»¥æ·»åŠ å¥åº·æ£€æŸ¥é€»è¾‘

# åˆ‡æ¢æµé‡
echo "åˆ‡æ¢æµé‡åˆ° $NEW_ENV ç¯å¢ƒ..."
sudo rm -f $CURRENT_LINK
sudo ln -s $NEW_DIR $CURRENT_LINK

# é‡å¯æœåŠ¡
sudo systemctl reload nginx

echo "éƒ¨ç½²å®Œæˆ! å½“å‰ç¯å¢ƒ: $NEW_ENV"
```

#### 2. æ»šåŠ¨æ›´æ–°
```bash
#!/bin/bash
# rolling-update.sh

SERVERS=("server1.company.com" "server2.company.com" "server3.company.com")
BUILD_FILE="$1"

for server in "${SERVERS[@]}"; do
    echo "æ›´æ–°æœåŠ¡å™¨: $server"
    
    # ä»è´Ÿè½½å‡è¡¡ä¸­ç§»é™¤
    # curl -X POST "http://load-balancer/api/remove-server/$server"
    
    # ç­‰å¾…è¿æ¥æ’ç©º
    sleep 30
    
    # éƒ¨ç½²æ–°ç‰ˆæœ¬
    scp $BUILD_FILE ubuntu@$server:/tmp/
    ssh ubuntu@$server "sudo /opt/scripts/deploy-local.sh /tmp/$BUILD_FILE"
    
    # å¥åº·æ£€æŸ¥
    for i in {1..10}; do
        if curl -f "http://$server/health"; then
            echo "æœåŠ¡å™¨ $server å¥åº·æ£€æŸ¥é€šè¿‡"
            break
        fi
        sleep 10
    done
    
    # é‡æ–°åŠ å…¥è´Ÿè½½å‡è¡¡
    # curl -X POST "http://load-balancer/api/add-server/$server"
    
    echo "æœåŠ¡å™¨ $server æ›´æ–°å®Œæˆ"
done
```

---

## ç›‘æ§å’Œæ—¥å¿—

### ğŸ“Š åº”ç”¨ç›‘æ§

#### 1. ç³»ç»Ÿç›‘æ§é…ç½®
```yaml
# docker-compose.monitoring.yml
version: '3.8'

services:
  # Prometheusç›‘æ§
  prometheus:
    image: prom/prometheus:latest
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus-data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'

  # Grafanaå¯è§†åŒ–
  grafana:
    image: grafana/grafana:latest
    ports:
      - "3001:3000"
    volumes:
      - grafana-data:/var/lib/grafana
      - ./grafana/dashboards:/var/lib/grafana/dashboards
      - ./grafana/provisioning:/etc/grafana/provisioning
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin123

  # Node Exporter
  node-exporter:
    image: prom/node-exporter:latest
    ports:
      - "9100:9100"
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
    command:
      - '--path.procfs=/host/proc'
      - '--path.rootfs=/rootfs'
      - '--path.sysfs=/host/sys'
      - '--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)'

volumes:
  prometheus-data:
  grafana-data:
```

#### 2. Prometheusé…ç½®
```yaml
# prometheus.yml
global:
  scrape_interval: 15s

scrape_configs:
  - job_name: 'prometheus'
    static_configs:
      - targets: ['localhost:9090']

  - job_name: 'node-exporter'
    static_configs:
      - targets: ['node-exporter:9100']

  - job_name: 'nginx'
    static_configs:
      - targets: ['nginx:9113']

  - job_name: 'logistics-app'
    static_configs:
      - targets: ['frontend:80']
    metrics_path: '/metrics'
```

#### 3. åº”ç”¨æ€§èƒ½ç›‘æ§
```typescript
// src/utils/monitoring.ts
export class AppMonitoring {
  private static instance: AppMonitoring;
  
  static getInstance(): AppMonitoring {
    if (!AppMonitoring.instance) {
      AppMonitoring.instance = new AppMonitoring();
    }
    return AppMonitoring.instance;
  }

  // è®°å½•é¡µé¢åŠ è½½æ—¶é—´
  trackPageLoad(pageName: string) {
    const navigationEntries = performance.getEntriesByType('navigation') as PerformanceNavigationTiming[];
    if (navigationEntries.length > 0) {
      const loadTime = navigationEntries[0].loadEventEnd - navigationEntries[0].loadEventStart;
      this.sendMetric('page_load_time', loadTime, { page: pageName });
    }
  }

  // è®°å½•APIè°ƒç”¨æ—¶é—´
  trackApiCall(endpoint: string, duration: number, status: number) {
    this.sendMetric('api_call_duration', duration, {
      endpoint,
      status: status.toString()
    });
  }

  // è®°å½•ç”¨æˆ·è¡Œä¸º
  trackUserAction(action: string, category: string = 'user') {
    this.sendEvent('user_action', {
      action,
      category,
      timestamp: Date.now()
    });
  }

  // è®°å½•é”™è¯¯
  trackError(error: Error, context?: any) {
    this.sendEvent('error', {
      message: error.message,
      stack: error.stack,
      context,
      timestamp: Date.now()
    });
  }

  private sendMetric(name: string, value: number, labels?: Record<string, string>) {
    // å‘é€åˆ°ç›‘æ§æœåŠ¡
    if (process.env.NODE_ENV === 'production') {
      fetch('/api/metrics', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ name, value, labels, timestamp: Date.now() })
      }).catch(console.error);
    }
  }

  private sendEvent(type: string, data: any) {
    // å‘é€äº‹ä»¶åˆ°åˆ†ææœåŠ¡
    if (process.env.NODE_ENV === 'production') {
      fetch('/api/events', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ type, data })
      }).catch(console.error);
    }
  }
}

// React Hook
export function useMonitoring() {
  const monitoring = AppMonitoring.getInstance();
  
  useEffect(() => {
    // ç›‘å¬å…¨å±€é”™è¯¯
    const handleError = (event: ErrorEvent) => {
      monitoring.trackError(event.error);
    };
    
    const handleUnhandledRejection = (event: PromiseRejectionEvent) => {
      monitoring.trackError(new Error(event.reason));
    };
    
    window.addEventListener('error', handleError);
    window.addEventListener('unhandledrejection', handleUnhandledRejection);
    
    return () => {
      window.removeEventListener('error', handleError);
      window.removeEventListener('unhandledrejection', handleUnhandledRejection);
    };
  }, [monitoring]);
  
  return monitoring;
}
```

### ğŸ“ æ—¥å¿—ç®¡ç†

#### 1. é›†ä¸­å¼æ—¥å¿—é…ç½®
```yaml
# docker-compose.logging.yml
version: '3.8'

services:
  # ELK Stack for logging
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.8.0
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false
    ports:
      - "9200:9200"
    volumes:
      - elasticsearch-data:/usr/share/elasticsearch/data

  logstash:
    image: docker.elastic.co/logstash/logstash:8.8.0
    ports:
      - "5000:5000"
    volumes:
      - ./logstash/pipeline:/usr/share/logstash/pipeline
      - ./logstash/config:/usr/share/logstash/config
    depends_on:
      - elasticsearch

  kibana:
    image: docker.elastic.co/kibana/kibana:8.8.0
    ports:
      - "5601:5601"
    environment:
      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200
    depends_on:
      - elasticsearch

  # Filebeat for log shipping
  filebeat:
    image: docker.elastic.co/beats/filebeat:8.8.0
    user: root
    volumes:
      - ./filebeat.yml:/usr/share/filebeat/filebeat.yml:ro
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
    depends_on:
      - elasticsearch

volumes:
  elasticsearch-data:
```

#### 2. åº”ç”¨æ—¥å¿—é…ç½®
```typescript
// src/utils/logger.ts
enum LogLevel {
  DEBUG = 0,
  INFO = 1,
  WARN = 2,
  ERROR = 3
}

class Logger {
  private level: LogLevel;
  private context: string;

  constructor(context: string = 'App') {
    this.context = context;
    this.level = process.env.NODE_ENV === 'production' ? LogLevel.INFO : LogLevel.DEBUG;
  }

  debug(message: string, data?: any) {
    if (this.level <= LogLevel.DEBUG) {
      this.log('DEBUG', message, data);
    }
  }

  info(message: string, data?: any) {
    if (this.level <= LogLevel.INFO) {
      this.log('INFO', message, data);
    }
  }

  warn(message: string, data?: any) {
    if (this.level <= LogLevel.WARN) {
      this.log('WARN', message, data);
    }
  }

  error(message: string, error?: Error | any) {
    this.log('ERROR', message, error);
    
    // ç”Ÿäº§ç¯å¢ƒå‘é€åˆ°é”™è¯¯è¿½è¸ªæœåŠ¡
    if (process.env.NODE_ENV === 'production') {
      this.sendToErrorService(message, error);
    }
  }

  private log(level: string, message: string, data?: any) {
    const timestamp = new Date().toISOString();
    const logEntry = {
      timestamp,
      level,
      context: this.context,
      message,
      data,
      ...(data instanceof Error && {
        stack: data.stack,
        name: data.name
      })
    };

    // æ§åˆ¶å°è¾“å‡º
    console.log(JSON.stringify(logEntry, null, 2));

    // å‘é€åˆ°æ—¥å¿—æœåŠ¡
    if (process.env.NODE_ENV === 'production') {
      this.sendToLogService(logEntry);
    }
  }

  private sendToLogService(logEntry: any) {
    // å‘é€åˆ°é›†ä¸­å¼æ—¥å¿—æœåŠ¡
    fetch('/api/logs', {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify(logEntry)
    }).catch(() => {
      // é™é»˜å¤„ç†æ—¥å¿—å‘é€å¤±è´¥
    });
  }

  private sendToErrorService(message: string, error: any) {
    // å‘é€åˆ°é”™è¯¯è¿½è¸ªæœåŠ¡ (å¦‚Sentry)
    if (window.Sentry) {
      window.Sentry.captureException(error instanceof Error ? error : new Error(message));
    }
  }
}

// åˆ›å»ºä¸åŒä¸Šä¸‹æ–‡çš„æ—¥å¿—å™¨
export const logger = new Logger('App');
export const apiLogger = new Logger('API');
export const authLogger = new Logger('Auth');
export const uiLogger = new Logger('UI');
```

#### 3. æ—¥å¿—è½®è½¬å’Œæ¸…ç†
```bash
#!/bin/bash
# log-rotation.sh

LOG_DIR="/var/log/logistics-app"
RETENTION_DAYS=30

# åˆ›å»ºæ—¥å¿—ç›®å½•
mkdir -p $LOG_DIR

# é…ç½®logrotate
cat > /etc/logrotate.d/logistics-app << EOF
$LOG_DIR/*.log {
    daily
    rotate 30
    compress
    delaycompress
    missingok
    notifempty
    create 0644 www-data www-data
    postrotate
        systemctl reload nginx > /dev/null 2>&1 || true
    endscript
}
EOF

# æ¸…ç†æ—§æ—¥å¿—
find $LOG_DIR -name "*.log.*" -mtime +$RETENTION_DAYS -delete

echo "æ—¥å¿—è½®è½¬é…ç½®å®Œæˆ"
```

---

## å¤‡ä»½ç­–ç•¥

### ğŸ’¾ æ•°æ®å¤‡ä»½

#### 1. å…¨é‡å¤‡ä»½è„šæœ¬
```bash
#!/bin/bash
# full-backup.sh

set -e

BACKUP_DIR="/opt/backups"
DATE=$(date +%Y%m%d_%H%M%S)
RETENTION_DAYS=90

# åˆ›å»ºå¤‡ä»½ç›®å½•
mkdir -p $BACKUP_DIR/{database,files,config}

echo "å¼€å§‹å…¨é‡å¤‡ä»½..."

# æ•°æ®åº“å¤‡ä»½
echo "å¤‡ä»½æ•°æ®åº“..."
pg_dump $DATABASE_URL | gzip > $BACKUP_DIR/database/db_backup_$DATE.sql.gz

# æ–‡ä»¶å¤‡ä»½
echo "å¤‡ä»½åº”ç”¨æ–‡ä»¶..."
tar -czf $BACKUP_DIR/files/app_backup_$DATE.tar.gz \
    /opt/logistics-app \
    --exclude='*.log' \
    --exclude='node_modules' \
    --exclude='dist'

# é…ç½®æ–‡ä»¶å¤‡ä»½
echo "å¤‡ä»½é…ç½®æ–‡ä»¶..."
tar -czf $BACKUP_DIR/config/config_backup_$DATE.tar.gz \
    /etc/nginx \
    /etc/ssl \
    /opt/logistics-app/.env* \
    /opt/logistics-app/docker-compose*.yml

# ä¸Šä¼ åˆ°äº‘å­˜å‚¨
if [ ! -z "$AWS_S3_BUCKET" ]; then
    echo "ä¸Šä¼ åˆ°S3..."
    aws s3 sync $BACKUP_DIR s3://$AWS_S3_BUCKET/backups/$(date +%Y/%m/%d)/
fi

# æ¸…ç†æ—§å¤‡ä»½
find $BACKUP_DIR -name "*backup_*" -mtime +$RETENTION_DAYS -delete

# ç”Ÿæˆå¤‡ä»½æŠ¥å‘Š
cat > $BACKUP_DIR/backup_report_$DATE.txt << EOF
å¤‡ä»½å®Œæˆæ—¶é—´: $(date)
æ•°æ®åº“å¤‡ä»½: db_backup_$DATE.sql.gz
åº”ç”¨æ–‡ä»¶å¤‡ä»½: app_backup_$DATE.tar.gz
é…ç½®æ–‡ä»¶å¤‡ä»½: config_backup_$DATE.tar.gz
å¤‡ä»½å¤§å°: $(du -sh $BACKUP_DIR | cut -f1)
EOF

echo "å…¨é‡å¤‡ä»½å®Œæˆ!"
```

#### 2. å¢é‡å¤‡ä»½è„šæœ¬
```bash
#!/bin/bash
# incremental-backup.sh

set -e

BACKUP_DIR="/opt/backups/incremental"
DATE=$(date +%Y%m%d_%H%M%S)
LAST_BACKUP_FILE="$BACKUP_DIR/.last_backup"

mkdir -p $BACKUP_DIR

echo "å¼€å§‹å¢é‡å¤‡ä»½..."

# è·å–ä¸Šæ¬¡å¤‡ä»½æ—¶é—´
if [ -f "$LAST_BACKUP_FILE" ]; then
    LAST_BACKUP=$(cat $LAST_BACKUP_FILE)
    echo "ä¸Šæ¬¡å¤‡ä»½æ—¶é—´: $LAST_BACKUP"
else
    LAST_BACKUP="1970-01-01 00:00:00"
    echo "é¦–æ¬¡å¢é‡å¤‡ä»½"
fi

# å¤‡ä»½å˜æ›´çš„æ–‡ä»¶
find /opt/logistics-app -type f -newer "$LAST_BACKUP_FILE" 2>/dev/null | \
    tar -czf $BACKUP_DIR/incremental_$DATE.tar.gz -T -

# æ•°æ®åº“å¢é‡å¤‡ä»½ (WALæ—¥å¿—)
if command -v pg_receivewal &> /dev/null; then
    pg_receivewal -D $BACKUP_DIR/wal_$DATE --slot=backup_slot --create-slot
fi

# æ›´æ–°å¤‡ä»½æ—¶é—´æˆ³
date > $LAST_BACKUP_FILE

echo "å¢é‡å¤‡ä»½å®Œæˆ!"
```

#### 3. å¤‡ä»½éªŒè¯è„šæœ¬
```bash
#!/bin/bash
# verify-backup.sh

BACKUP_FILE="$1"

if [ -z "$BACKUP_FILE" ]; then
    echo "ç”¨æ³•: $0 <backup-file>"
    exit 1
fi

echo "éªŒè¯å¤‡ä»½æ–‡ä»¶: $BACKUP_FILE"

# æ£€æŸ¥æ–‡ä»¶å®Œæ•´æ€§
if file $BACKUP_FILE | grep -q "gzip compressed"; then
    echo "âœ“ æ–‡ä»¶æ ¼å¼æ­£ç¡®"
    
    # æµ‹è¯•è§£å‹
    if gunzip -t $BACKUP_FILE 2>/dev/null; then
        echo "âœ“ æ–‡ä»¶å®Œæ•´æ€§éªŒè¯é€šè¿‡"
    else
        echo "âœ— æ–‡ä»¶æŸå"
        exit 1
    fi
else
    echo "âœ— æ–‡ä»¶æ ¼å¼ä¸æ­£ç¡®"
    exit 1
fi

# å¦‚æœæ˜¯æ•°æ®åº“å¤‡ä»½ï¼Œæµ‹è¯•SQLè¯­æ³•
if echo $BACKUP_FILE | grep -q "db_backup"; then
    echo "éªŒè¯æ•°æ®åº“å¤‡ä»½..."
    
    # åˆ›å»ºä¸´æ—¶æ•°æ®åº“è¿›è¡ŒéªŒè¯
    TEMP_DB="temp_verify_$(date +%s)"
    createdb $TEMP_DB
    
    if gunzip -c $BACKUP_FILE | psql $TEMP_DB >/dev/null 2>&1; then
        echo "âœ“ æ•°æ®åº“å¤‡ä»½éªŒè¯é€šè¿‡"
    else
        echo "âœ— æ•°æ®åº“å¤‡ä»½éªŒè¯å¤±è´¥"
        dropdb $TEMP_DB
        exit 1
    fi
    
    dropdb $TEMP_DB
fi

echo "å¤‡ä»½éªŒè¯å®Œæˆ!"
```

### ğŸ“‹ å¤‡ä»½ç›‘æ§

#### 1. å¤‡ä»½çŠ¶æ€æ£€æŸ¥
```bash
#!/bin/bash
# backup-monitor.sh

BACKUP_DIR="/opt/backups"
ALERT_EMAIL="admin@company.com"
MAX_AGE_HOURS=25

echo "æ£€æŸ¥å¤‡ä»½çŠ¶æ€..."

# æ£€æŸ¥æœ€æ–°å¤‡ä»½æ—¶é—´
LATEST_BACKUP=$(find $BACKUP_DIR -name "*backup_*.gz" -type f -printf '%T@ %p\n' | sort -n | tail -1)

if [ -z "$LATEST_BACKUP" ]; then
    echo "è­¦å‘Š: æœªæ‰¾åˆ°å¤‡ä»½æ–‡ä»¶"
    # å‘é€å‘Šè­¦é‚®ä»¶
    mail -s "å¤‡ä»½å‘Šè­¦: æœªæ‰¾åˆ°å¤‡ä»½æ–‡ä»¶" $ALERT_EMAIL < /dev/null
    exit 1
fi

BACKUP_TIME=$(echo $LATEST_BACKUP | cut -d' ' -f1)
BACKUP_FILE=$(echo $LATEST_BACKUP | cut -d' ' -f2-)
CURRENT_TIME=$(date +%s)
AGE_HOURS=$(( (CURRENT_TIME - BACKUP_TIME) / 3600 ))

echo "æœ€æ–°å¤‡ä»½: $BACKUP_FILE"
echo "å¤‡ä»½æ—¶é—´: $(date -d @$BACKUP_TIME)"
echo "è·ä»Šæ—¶é—´: ${AGE_HOURS}å°æ—¶"

if [ $AGE_HOURS -gt $MAX_AGE_HOURS ]; then
    echo "è­¦å‘Š: å¤‡ä»½è¿‡æœŸ (${AGE_HOURS}å°æ—¶å‰)"
    mail -s "å¤‡ä»½å‘Šè­¦: å¤‡ä»½è¿‡æœŸ" $ALERT_EMAIL << EOF
æœ€æ–°å¤‡ä»½æ—¶é—´: $(date -d @$BACKUP_TIME)
è·ä»Šæ—¶é—´: ${AGE_HOURS}å°æ—¶
å¤‡ä»½æ–‡ä»¶: $BACKUP_FILE

è¯·æ£€æŸ¥å¤‡ä»½ç³»ç»ŸçŠ¶æ€ã€‚
EOF
    exit 1
fi

echo "å¤‡ä»½çŠ¶æ€æ­£å¸¸"
```

---

## ç»´æŠ¤ä»»åŠ¡

### ğŸ”§ å®šæœŸç»´æŠ¤

#### 1. ç³»ç»Ÿæ¸…ç†è„šæœ¬
```bash
#!/bin/bash
# system-cleanup.sh

echo "å¼€å§‹ç³»ç»Ÿæ¸…ç†..."

# æ¸…ç†Docker
echo "æ¸…ç†Dockerèµ„æº..."
docker system prune -f
docker volume prune -f
docker image prune -f

# æ¸…ç†æ—¥å¿—æ–‡ä»¶
echo "æ¸…ç†ç³»ç»Ÿæ—¥å¿—..."
journalctl --vacuum-time=30d
find /var/log -name "*.log" -mtime +30 -delete
find /var/log -name "*.log.*" -mtime +30 -delete

# æ¸…ç†ä¸´æ—¶æ–‡ä»¶
echo "æ¸…ç†ä¸´æ—¶æ–‡ä»¶..."
find /tmp -type f -atime +7 -delete
find /var/tmp -type f -atime +7 -delete

# æ¸…ç†åŒ…ç¼“å­˜
echo "æ¸…ç†åŒ…ç®¡ç†å™¨ç¼“å­˜..."
apt-get autoremove -y
apt-get autoclean

# æ¸…ç†åº”ç”¨ç¼“å­˜
echo "æ¸…ç†åº”ç”¨ç¼“å­˜..."
rm -rf /opt/logistics-app/node_modules/.cache
rm -rf /opt/logistics-app/dist/.cache

# æ£€æŸ¥ç£ç›˜ç©ºé—´
echo "ç£ç›˜ä½¿ç”¨æƒ…å†µ:"
df -h

echo "ç³»ç»Ÿæ¸…ç†å®Œæˆ!"
```

#### 2. æ€§èƒ½ä¼˜åŒ–è„šæœ¬
```bash
#!/bin/bash
# performance-tune.sh

echo "å¼€å§‹æ€§èƒ½ä¼˜åŒ–..."

# ä¼˜åŒ–æ•°æ®åº“
echo "ä¼˜åŒ–æ•°æ®åº“æ€§èƒ½..."
psql $DATABASE_URL << EOF
-- æ›´æ–°è¡¨ç»Ÿè®¡ä¿¡æ¯
ANALYZE;

-- é‡å»ºç´¢å¼•
REINDEX DATABASE postgres;

-- æ¸…ç†æ­»å…ƒç»„
VACUUM ANALYZE;

-- æ£€æŸ¥æ…¢æŸ¥è¯¢
SELECT query, mean_exec_time, calls
FROM pg_stat_statements
WHERE mean_exec_time > 1000
ORDER BY mean_exec_time DESC
LIMIT 10;
EOF

# ä¼˜åŒ–Nginx
echo "ä¼˜åŒ–Nginxé…ç½®..."
nginx -t && systemctl reload nginx

# æ£€æŸ¥ç³»ç»Ÿèµ„æº
echo "ç³»ç»Ÿèµ„æºä½¿ç”¨æƒ…å†µ:"
echo "CPUä½¿ç”¨ç‡:"
top -bn1 | grep "Cpu(s)" | awk '{print $2}' | sed 's/%us,//'

echo "å†…å­˜ä½¿ç”¨ç‡:"
free -m | awk 'NR==2{printf "%.2f%%\n", $3*100/$2}'

echo "ç£ç›˜I/O:"
iostat -x 1 1 | tail -n +4

echo "æ€§èƒ½ä¼˜åŒ–å®Œæˆ!"
```

#### 3. å®‰å…¨æ›´æ–°è„šæœ¬
```bash
#!/bin/bash
# security-update.sh

echo "å¼€å§‹å®‰å…¨æ›´æ–°..."

# æ›´æ–°ç³»ç»ŸåŒ…
echo "æ›´æ–°ç³»ç»ŸåŒ…..."
apt-get update
apt-get upgrade -y

# æ›´æ–°Dockeré•œåƒ
echo "æ›´æ–°Dockeré•œåƒ..."
docker-compose pull
docker-compose up -d

# æ£€æŸ¥SSLè¯ä¹¦
echo "æ£€æŸ¥SSLè¯ä¹¦..."
CERT_FILE="/etc/ssl/logistics.company.com.crt"
if [ -f "$CERT_FILE" ]; then
    EXPIRE_DATE=$(openssl x509 -enddate -noout -in $CERT_FILE | cut -d= -f2)
    EXPIRE_TIMESTAMP=$(date -d "$EXPIRE_DATE" +%s)
    CURRENT_TIMESTAMP=$(date +%s)
    DAYS_LEFT=$(( (EXPIRE_TIMESTAMP - CURRENT_TIMESTAMP) / 86400 ))
    
    echo "SSLè¯ä¹¦å‰©ä½™å¤©æ•°: $DAYS_LEFT"
    
    if [ $DAYS_LEFT -lt 30 ]; then
        echo "è­¦å‘Š: SSLè¯ä¹¦å³å°†è¿‡æœŸ!"
        # è‡ªåŠ¨ç»­æœŸè¯ä¹¦
        certbot renew --nginx
    fi
fi

# æ£€æŸ¥å®‰å…¨æ¼æ´
echo "æ£€æŸ¥å®‰å…¨æ¼æ´..."
if command -v npm &> /dev/null; then
    cd /opt/logistics-app
    npm audit --audit-level=high
fi

echo "å®‰å…¨æ›´æ–°å®Œæˆ!"
```

### ğŸ“Š å¥åº·æ£€æŸ¥

#### 1. æœåŠ¡å¥åº·æ£€æŸ¥
```bash
#!/bin/bash
# health-check.sh

SERVICES=("nginx" "docker" "logistics-app")
ENDPOINTS=(
    "https://logistics.company.com/health"
    "https://logistics.company.com/api/health"
)

echo "å¼€å§‹å¥åº·æ£€æŸ¥..."

# æ£€æŸ¥ç³»ç»ŸæœåŠ¡
for service in "${SERVICES[@]}"; do
    if systemctl is-active --quiet $service; then
        echo "âœ“ $service æœåŠ¡æ­£å¸¸"
    else
        echo "âœ— $service æœåŠ¡å¼‚å¸¸"
        systemctl status $service
    fi
done

# æ£€æŸ¥HTTPç«¯ç‚¹
for endpoint in "${ENDPOINTS[@]}"; do
    if curl -f -s $endpoint > /dev/null; then
        echo "âœ“ $endpoint å¯è®¿é—®"
    else
        echo "âœ— $endpoint ä¸å¯è®¿é—®"
    fi
done

# æ£€æŸ¥æ•°æ®åº“è¿æ¥
if psql $DATABASE_URL -c "SELECT 1;" > /dev/null 2>&1; then
    echo "âœ“ æ•°æ®åº“è¿æ¥æ­£å¸¸"
else
    echo "âœ— æ•°æ®åº“è¿æ¥å¼‚å¸¸"
fi

# æ£€æŸ¥ç£ç›˜ç©ºé—´
DISK_USAGE=$(df / | awk 'NR==2 {print $5}' | sed 's/%//')
if [ $DISK_USAGE -lt 90 ]; then
    echo "âœ“ ç£ç›˜ç©ºé—´å……è¶³ (${DISK_USAGE}%)"
else
    echo "âœ— ç£ç›˜ç©ºé—´ä¸è¶³ (${DISK_USAGE}%)"
fi

# æ£€æŸ¥å†…å­˜ä½¿ç”¨
MEMORY_USAGE=$(free | awk 'NR==2{printf "%.0f", $3*100/$2}')
if [ $MEMORY_USAGE -lt 90 ]; then
    echo "âœ“ å†…å­˜ä½¿ç”¨æ­£å¸¸ (${MEMORY_USAGE}%)"
else
    echo "âœ— å†…å­˜ä½¿ç”¨è¿‡é«˜ (${MEMORY_USAGE}%)"
fi

echo "å¥åº·æ£€æŸ¥å®Œæˆ!"
```

#### 2. è‡ªåŠ¨åŒ–å·¡æ£€
```bash
#!/bin/bash
# auto-patrol.sh

REPORT_FILE="/tmp/patrol_report_$(date +%Y%m%d_%H%M%S).txt"
ALERT_EMAIL="admin@company.com"

echo "å¼€å§‹è‡ªåŠ¨åŒ–å·¡æ£€..." | tee $REPORT_FILE

# æ‰§è¡Œå„é¡¹æ£€æŸ¥
echo "=== æœåŠ¡çŠ¶æ€æ£€æŸ¥ ===" | tee -a $REPORT_FILE
/opt/scripts/health-check.sh | tee -a $REPORT_FILE

echo "=== æ€§èƒ½æŒ‡æ ‡æ£€æŸ¥ ===" | tee -a $REPORT_FILE
echo "CPUè´Ÿè½½: $(uptime | awk -F'load average:' '{print $2}')" | tee -a $REPORT_FILE
echo "å†…å­˜ä½¿ç”¨: $(free -h | awk 'NR==2{print $3"/"$2}')" | tee -a $REPORT_FILE
echo "ç£ç›˜ä½¿ç”¨: $(df -h / | awk 'NR==2{print $5}')" | tee -a $REPORT_FILE

echo "=== æ—¥å¿—é”™è¯¯æ£€æŸ¥ ===" | tee -a $REPORT_FILE
ERROR_COUNT=$(journalctl --since="1 hour ago" --priority=err | wc -l)
echo "æœ€è¿‘1å°æ—¶é”™è¯¯æ—¥å¿—æ•°: $ERROR_COUNT" | tee -a $REPORT_FILE

echo "=== ç½‘ç»œè¿é€šæ€§æ£€æŸ¥ ===" | tee -a $REPORT_FILE
if ping -c 3 8.8.8.8 > /dev/null; then
    echo "âœ“ å¤–ç½‘è¿é€šæ­£å¸¸" | tee -a $REPORT_FILE
else
    echo "âœ— å¤–ç½‘è¿é€šå¼‚å¸¸" | tee -a $REPORT_FILE
fi

# ç”Ÿæˆå·¡æ£€æŠ¥å‘Š
echo "å·¡æ£€æ—¶é—´: $(date)" >> $REPORT_FILE
echo "ç³»ç»Ÿè¿è¡Œæ—¶é—´: $(uptime -p)" >> $REPORT_FILE

# å‘é€æŠ¥å‘Š
if [ -s $REPORT_FILE ]; then
    mail -s "ç³»ç»Ÿå·¡æ£€æŠ¥å‘Š - $(date +%Y-%m-%d)" $ALERT_EMAIL < $REPORT_FILE
fi

echo "è‡ªåŠ¨åŒ–å·¡æ£€å®Œæˆ!"
```

---

## æ•…éšœæ’é™¤

### ğŸš¨ å¸¸è§é—®é¢˜è§£å†³

#### 1. åº”ç”¨æ— æ³•è®¿é—®
```bash
#!/bin/bash
# troubleshoot-app.sh

echo "æ’æŸ¥åº”ç”¨è®¿é—®é—®é¢˜..."

# æ£€æŸ¥NginxçŠ¶æ€
echo "1. æ£€æŸ¥NginxçŠ¶æ€"
if systemctl is-active --quiet nginx; then
    echo "âœ“ Nginxè¿è¡Œæ­£å¸¸"
    nginx -t
else
    echo "âœ— Nginxæœªè¿è¡Œ"
    systemctl status nginx
    echo "å°è¯•å¯åŠ¨Nginx..."
    systemctl start nginx
fi

# æ£€æŸ¥ç«¯å£ç›‘å¬
echo "2. æ£€æŸ¥ç«¯å£ç›‘å¬"
netstat -tlnp | grep -E ':(80|443)'

# æ£€æŸ¥é˜²ç«å¢™
echo "3. æ£€æŸ¥é˜²ç«å¢™è§„åˆ™"
ufw status

# æ£€æŸ¥SSLè¯ä¹¦
echo "4. æ£€æŸ¥SSLè¯ä¹¦"
if [ -f "/etc/ssl/logistics.company.com.crt" ]; then
    openssl x509 -in /etc/ssl/logistics.company.com.crt -text -noout | grep -A 2 "Validity"
else
    echo "SSLè¯ä¹¦æ–‡ä»¶ä¸å­˜åœ¨"
fi

# æ£€æŸ¥DNSè§£æ
echo "5. æ£€æŸ¥DNSè§£æ"
nslookup logistics.company.com

# æ£€æŸ¥åº”ç”¨æ—¥å¿—
echo "6. æ£€æŸ¥åº”ç”¨æ—¥å¿—"
tail -n 50 /var/log/nginx/error.log
```

#### 2. æ•°æ®åº“è¿æ¥é—®é¢˜
```bash
#!/bin/bash
# troubleshoot-db.sh

echo "æ’æŸ¥æ•°æ®åº“è¿æ¥é—®é¢˜..."

# æµ‹è¯•æ•°æ®åº“è¿æ¥
echo "1. æµ‹è¯•æ•°æ®åº“è¿æ¥"
if psql $DATABASE_URL -c "SELECT version();" > /dev/null 2>&1; then
    echo "âœ“ æ•°æ®åº“è¿æ¥æ­£å¸¸"
else
    echo "âœ— æ•°æ®åº“è¿æ¥å¤±è´¥"
    echo "é”™è¯¯ä¿¡æ¯:"
    psql $DATABASE_URL -c "SELECT version();" 2>&1
fi

# æ£€æŸ¥è¿æ¥æ± çŠ¶æ€
echo "2. æ£€æŸ¥è¿æ¥æ± çŠ¶æ€"
psql $DATABASE_URL -c "
    SELECT 
        state,
        count(*) 
    FROM pg_stat_activity 
    GROUP BY state;
"

# æ£€æŸ¥é•¿æ—¶é—´è¿è¡Œçš„æŸ¥è¯¢
echo "3. æ£€æŸ¥é•¿æ—¶é—´è¿è¡Œçš„æŸ¥è¯¢"
psql $DATABASE_URL -c "
    SELECT 
        pid,
        now() - pg_stat_activity.query_start AS duration,
        query 
    FROM pg_stat_activity 
    WHERE (now() - pg_stat_activity.query_start) > interval '5 minutes'
    AND state = 'active';
"

# æ£€æŸ¥é”ç­‰å¾…
echo "4. æ£€æŸ¥é”ç­‰å¾…"
psql $DATABASE_URL -c "
    SELECT 
        blocked_locks.pid AS blocked_pid,
        blocked_activity.usename AS blocked_user,
        blocking_locks.pid AS blocking_pid,
        blocking_activity.usename AS blocking_user,
        blocked_activity.query AS blocked_statement,
        blocking_activity.query AS current_statement_in_blocking_process
    FROM pg_catalog.pg_locks blocked_locks
    JOIN pg_catalog.pg_stat_activity blocked_activity ON blocked_activity.pid = blocked_locks.pid
    JOIN pg_catalog.pg_locks blocking_locks ON blocking_locks.locktype = blocked_locks.locktype
        AND blocking_locks.database IS NOT DISTINCT FROM blocked_locks.database
        AND blocking_locks.relation IS NOT DISTINCT FROM blocked_locks.relation
        AND blocking_locks.page IS NOT DISTINCT FROM blocked_locks.page
        AND blocking_locks.tuple IS NOT DISTINCT FROM blocked_locks.tuple
        AND blocking_locks.virtualxid IS NOT DISTINCT FROM blocked_locks.virtualxid
        AND blocking_locks.transactionid IS NOT DISTINCT FROM blocked_locks.transactionid
        AND blocking_locks.classid IS NOT DISTINCT FROM blocked_locks.classid
        AND blocking_locks.objid IS NOT DISTINCT FROM blocked_locks.objid
        AND blocking_locks.objsubid IS NOT DISTINCT FROM blocked_locks.objsubid
        AND blocking_locks.pid != blocked_locks.pid
    JOIN pg_catalog.pg_stat_activity blocking_activity ON blocking_activity.pid = blocking_locks.pid
    WHERE NOT blocked_locks.granted;
"
```

#### 3. æ€§èƒ½é—®é¢˜æ’æŸ¥
```bash
#!/bin/bash
# troubleshoot-performance.sh

echo "æ’æŸ¥æ€§èƒ½é—®é¢˜..."

# æ£€æŸ¥ç³»ç»Ÿè´Ÿè½½
echo "1. ç³»ç»Ÿè´Ÿè½½æƒ…å†µ"
uptime
echo "CPUä½¿ç”¨ç‡:"
top -bn1 | head -3

# æ£€æŸ¥å†…å­˜ä½¿ç”¨
echo "2. å†…å­˜ä½¿ç”¨æƒ…å†µ"
free -h
echo "å†…å­˜å ç”¨å‰10çš„è¿›ç¨‹:"
ps aux --sort=-%mem | head -10

# æ£€æŸ¥ç£ç›˜I/O
echo "3. ç£ç›˜I/Oæƒ…å†µ"
iostat -x 1 3

# æ£€æŸ¥ç½‘ç»œè¿æ¥
echo "4. ç½‘ç»œè¿æ¥æƒ…å†µ"
ss -tuln | wc -l
echo "è¿æ¥æ•°æœ€å¤šçš„IP:"
ss -tn | awk 'NR>1 {print $4}' | cut -d: -f1 | sort | uniq -c | sort -nr | head -10

# æ£€æŸ¥æ…¢æŸ¥è¯¢
echo "5. æ•°æ®åº“æ…¢æŸ¥è¯¢"
psql $DATABASE_URL -c "
    SELECT 
        query,
        mean_exec_time,
        calls,
        total_exec_time
    FROM pg_stat_statements 
    WHERE mean_exec_time > 1000 
    ORDER BY mean_exec_time DESC 
    LIMIT 10;
"

# æ£€æŸ¥åº”ç”¨å“åº”æ—¶é—´
echo "6. åº”ç”¨å“åº”æ—¶é—´"
for endpoint in "/" "/api/health"; do
    response_time=$(curl -o /dev/null -s -w "%{time_total}" "https://logistics.company.com$endpoint")
    echo "$endpoint: ${response_time}s"
done
```

### ğŸ”§ è‡ªåŠ¨æ¢å¤è„šæœ¬

#### 1. æœåŠ¡è‡ªåŠ¨é‡å¯
```bash
#!/bin/bash
# auto-recovery.sh

SERVICES=("nginx" "docker" "logistics-app")
RESTART_THRESHOLD=3
RESTART_COUNT_FILE="/tmp/restart_count"

# åˆå§‹åŒ–é‡å¯è®¡æ•°
if [ ! -f $RESTART_COUNT_FILE ]; then
    echo "0" > $RESTART_COUNT_FILE
fi

RESTART_COUNT=$(cat $RESTART_COUNT_FILE)

echo "å¼€å§‹æœåŠ¡å¥åº·æ£€æŸ¥å’Œè‡ªåŠ¨æ¢å¤..."

for service in "${SERVICES[@]}"; do
    if ! systemctl is-active --quiet $service; then
        echo "æ£€æµ‹åˆ° $service æœåŠ¡å¼‚å¸¸"
        
        if [ $RESTART_COUNT -lt $RESTART_THRESHOLD ]; then
            echo "å°è¯•é‡å¯ $service æœåŠ¡ (ç¬¬$((RESTART_COUNT+1))æ¬¡)"
            systemctl restart $service
            
            # ç­‰å¾…æœåŠ¡å¯åŠ¨
            sleep 30
            
            if systemctl is-active --quiet $service; then
                echo "âœ“ $service æœåŠ¡é‡å¯æˆåŠŸ"
                # é‡ç½®é‡å¯è®¡æ•°
                echo "0" > $RESTART_COUNT_FILE
            else
                echo "âœ— $service æœåŠ¡é‡å¯å¤±è´¥"
                RESTART_COUNT=$((RESTART_COUNT+1))
                echo $RESTART_COUNT > $RESTART_COUNT_FILE
            fi
        else
            echo "âš  $service æœåŠ¡é‡å¯æ¬¡æ•°è¶…è¿‡é˜ˆå€¼ï¼Œå‘é€å‘Šè­¦"
            # å‘é€å‘Šè­¦
            mail -s "æœåŠ¡å¼‚å¸¸å‘Šè­¦: $service" admin@company.com << EOF
æœåŠ¡ $service å¼‚å¸¸ä¸”é‡å¯å¤±è´¥è¶…è¿‡ $RESTART_THRESHOLD æ¬¡ã€‚
è¯·äººå·¥ä»‹å…¥å¤„ç†ã€‚

æœåŠ¡çŠ¶æ€:
$(systemctl status $service)
EOF
        fi
    else
        echo "âœ“ $service æœåŠ¡æ­£å¸¸"
    fi
done

echo "è‡ªåŠ¨æ¢å¤æ£€æŸ¥å®Œæˆ"
```

#### 2. ç£ç›˜ç©ºé—´æ¸…ç†
```bash
#!/bin/bash
# auto-cleanup.sh

DISK_THRESHOLD=85
CURRENT_USAGE=$(df / | awk 'NR==2 {print $5}' | sed 's/%//')

if [ $CURRENT_USAGE -gt $DISK_THRESHOLD ]; then
    echo "ç£ç›˜ä½¿ç”¨ç‡ ${CURRENT_USAGE}% è¶…è¿‡é˜ˆå€¼ ${DISK_THRESHOLD}%ï¼Œå¼€å§‹è‡ªåŠ¨æ¸…ç†..."
    
    # æ¸…ç†Docker
    echo "æ¸…ç†Dockerèµ„æº..."
    docker system prune -f
    docker volume prune -f
    
    # æ¸…ç†æ—¥å¿—
    echo "æ¸…ç†æ—§æ—¥å¿—..."
    find /var/log -name "*.log.*" -mtime +7 -delete
    journalctl --vacuum-time=7d
    
    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
    echo "æ¸…ç†ä¸´æ—¶æ–‡ä»¶..."
    find /tmp -type f -mtime +1 -delete
    
    # æ¸…ç†åº”ç”¨ç¼“å­˜
    echo "æ¸…ç†åº”ç”¨ç¼“å­˜..."
    rm -rf /opt/logistics-app/node_modules/.cache
    rm -rf /opt/logistics-app/dist/.cache
    
    # æ£€æŸ¥æ¸…ç†åçš„ä½¿ç”¨ç‡
    NEW_USAGE=$(df / | awk 'NR==2 {print $5}' | sed 's/%//')
    echo "æ¸…ç†åç£ç›˜ä½¿ç”¨ç‡: ${NEW_USAGE}%"
    
    if [ $NEW_USAGE -gt $DISK_THRESHOLD ]; then
        echo "æ¸…ç†åä»è¶…è¿‡é˜ˆå€¼ï¼Œå‘é€å‘Šè­¦"
        mail -s "ç£ç›˜ç©ºé—´å‘Šè­¦" admin@company.com << EOF
ç£ç›˜ä½¿ç”¨ç‡: ${NEW_USAGE}%
æ¸…ç†å‰ä½¿ç”¨ç‡: ${CURRENT_USAGE}%

è¯·æ£€æŸ¥ç£ç›˜ä½¿ç”¨æƒ…å†µå¹¶é‡‡å–è¿›ä¸€æ­¥æªæ–½ã€‚
EOF
    fi
else
    echo "ç£ç›˜ä½¿ç”¨ç‡ ${CURRENT_USAGE}% æ­£å¸¸"
fi
```

---

## å®‰å…¨é…ç½®

### ğŸ”’ ç³»ç»Ÿå®‰å…¨åŠ å›º

#### 1. é˜²ç«å¢™é…ç½®
```bash
#!/bin/bash
# security-hardening.sh

echo "å¼€å§‹ç³»ç»Ÿå®‰å…¨åŠ å›º..."

# é…ç½®UFWé˜²ç«å¢™
echo "é…ç½®é˜²ç«å¢™è§„åˆ™..."
ufw --force reset
ufw default deny incoming
ufw default allow outgoing

# å…è®¸SSH (é™åˆ¶IPèŒƒå›´)
ufw allow from 192.168.1.0/24 to any port 22
ufw allow from 10.0.0.0/8 to any port 22

# å…è®¸HTTP/HTTPS
ufw allow 80
ufw allow 443

# å…è®¸ç‰¹å®šæœåŠ¡ç«¯å£
ufw allow from 127.0.0.1 to any port 5432  # PostgreSQL
ufw allow from 127.0.0.1 to any port 6379  # Redis

# å¯ç”¨é˜²ç«å¢™
ufw --force enable

# é…ç½®fail2ban
echo "é…ç½®fail2ban..."
apt-get install -y fail2ban

cat > /etc/fail2ban/jail.local << EOF
[DEFAULT]
bantime = 3600
findtime = 600
maxretry = 5
ignoreip = 127.0.0.1/8 192.168.1.0/24

[sshd]
enabled = true
port = ssh
filter = sshd
logpath = /var/log/auth.log

[nginx-http-auth]
enabled = true
filter = nginx-http-auth
port = http,https
logpath = /var/log/nginx/error.log

[nginx-limit-req]
enabled = true
filter = nginx-limit-req
port = http,https
logpath = /var/log/nginx/error.log
maxretry = 10
EOF

systemctl enable fail2ban
systemctl start fail2ban

echo "ç³»ç»Ÿå®‰å…¨åŠ å›ºå®Œæˆ"
```

#### 2. SSL/TLSé…ç½®
```bash
#!/bin/bash
# ssl-setup.sh

DOMAIN="logistics.company.com"
EMAIL="admin@company.com"

echo "é…ç½®SSLè¯ä¹¦..."

# å®‰è£…Certbot
apt-get update
apt-get install -y certbot python3-certbot-nginx

# ç”³è¯·Let's Encryptè¯ä¹¦
certbot --nginx -d $DOMAIN --email $EMAIL --agree-tos --non-interactive

# é…ç½®è‡ªåŠ¨ç»­æœŸ
cat > /etc/cron.d/certbot-renew << EOF
0 12 * * * root certbot renew --quiet && systemctl reload nginx
EOF

# é…ç½®å¼ºSSLè®¾ç½®
cat > /etc/nginx/conf.d/ssl.conf << EOF
# SSLé…ç½®
ssl_protocols TLSv1.2 TLSv1.3;
ssl_ciphers ECDHE-RSA-AES256-GCM-SHA512:DHE-RSA-AES256-GCM-SHA512:ECDHE-RSA-AES256-GCM-SHA384:DHE-RSA-AES256-GCM-SHA384;
ssl_prefer_server_ciphers off;
ssl_session_cache shared:SSL:10m;
ssl_session_timeout 10m;
ssl_stapling on;
ssl_stapling_verify on;

# å®‰å…¨å¤´
add_header Strict-Transport-Security "max-age=31536000; includeSubDomains" always;
add_header X-Frame-Options "SAMEORIGIN" always;
add_header X-Content-Type-Options "nosniff" always;
add_header X-XSS-Protection "1; mode=block" always;
add_header Referrer-Policy "no-referrer-when-downgrade" always;
EOF

nginx -t && systemctl reload nginx

echo "SSLé…ç½®å®Œæˆ"
```

#### 3. åº”ç”¨å®‰å…¨é…ç½®
```typescript
// src/utils/security.ts
export class SecurityManager {
  // CSPç­–ç•¥é…ç½®
  static getCSPPolicy(): string {
    return [
      "default-src 'self'",
      "script-src 'self' 'unsafe-inline' 'unsafe-eval' https://cdn.jsdelivr.net",
      "style-src 'self' 'unsafe-inline' https://fonts.googleapis.com",
      "font-src 'self' https://fonts.gstatic.com",
      "img-src 'self' data: https:",
      "connect-src 'self' https://your-project.supabase.co wss://your-project.supabase.co",
      "media-src 'self'",
      "object-src 'none'",
      "base-uri 'self'",
      "form-action 'self'",
      "frame-ancestors 'none'"
    ].join('; ');
  }

  // è¾“å…¥éªŒè¯
  static sanitizeInput(input: string): string {
    return input
      .replace(/[<>]/g, '') // ç§»é™¤HTMLæ ‡ç­¾
      .replace(/['"]/g, '') // ç§»é™¤å¼•å·
      .trim();
  }

  // SQLæ³¨å…¥é˜²æŠ¤
  static validateSQLInput(input: string): boolean {
    const sqlPatterns = [
      /(\b(SELECT|INSERT|UPDATE|DELETE|DROP|CREATE|ALTER|EXEC|UNION|SCRIPT)\b)/i,
      /[';--]/,
      /\/\*.*\*\//
    ];
    
    return !sqlPatterns.some(pattern => pattern.test(input));
  }

  // XSSé˜²æŠ¤
  static escapeHtml(text: string): string {
    const map: { [key: string]: string } = {
      '&': '&amp;',
      '<': '&lt;',
      '>': '&gt;',
      '"': '&quot;',
      "'": '&#039;'
    };
    
    return text.replace(/[&<>"']/g, (m) => map[m]);
  }

  // æ•æ„Ÿæ•°æ®è„±æ•
  static maskSensitiveData(data: string, type: 'phone' | 'email' | 'id'): string {
    switch (type) {
      case 'phone':
        return data.replace(/(\d{3})\d{4}(\d{4})/, '$1****$2');
      case 'email':
        return data.replace(/(.{2}).*(@.*)/, '$1***$2');
      case 'id':
        return data.replace(/(.{4}).*(.{4})/, '$1****$2');
      default:
        return data;
    }
  }

  // ç”Ÿæˆå®‰å…¨ä»¤ç‰Œ
  static generateSecureToken(length: number = 32): string {
    const chars = 'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789';
    let result = '';
    
    for (let i = 0; i < length; i++) {
      result += chars.charAt(Math.floor(Math.random() * chars.length));
    }
    
    return result;
  }

  // é€Ÿç‡é™åˆ¶æ£€æŸ¥
  static checkRateLimit(key: string, limit: number, window: number): boolean {
    const now = Date.now();
    const windowStart = now - window;
    
    // ä»localStorageè·å–è¯·æ±‚è®°å½•
    const requests = JSON.parse(localStorage.getItem(`rate_limit_${key}`) || '[]');
    
    // è¿‡æ»¤æ‰çª—å£å¤–çš„è¯·æ±‚
    const validRequests = requests.filter((timestamp: number) => timestamp > windowStart);
    
    if (validRequests.length >= limit) {
      return false; // è¶…è¿‡é™åˆ¶
    }
    
    // è®°å½•å½“å‰è¯·æ±‚
    validRequests.push(now);
    localStorage.setItem(`rate_limit_${key}`, JSON.stringify(validRequests));
    
    return true;
  }
}
```

### ğŸ›¡ï¸ å®‰å…¨ç›‘æ§

#### 1. å…¥ä¾µæ£€æµ‹
```bash
#!/bin/bash
# intrusion-detection.sh

LOG_FILE="/var/log/security-monitor.log"
ALERT_EMAIL="security@company.com"

echo "å¼€å§‹å…¥ä¾µæ£€æµ‹æ‰«æ..." | tee -a $LOG_FILE

# æ£€æŸ¥å¼‚å¸¸ç™»å½•
echo "æ£€æŸ¥å¼‚å¸¸ç™»å½•..." | tee -a $LOG_FILE
FAILED_LOGINS=$(grep "Failed password" /var/log/auth.log | wc -l)
if [ $FAILED_LOGINS -gt 50 ]; then
    echo "è­¦å‘Š: æ£€æµ‹åˆ°å¼‚å¸¸ç™»å½•å°è¯• ($FAILED_LOGINS æ¬¡)" | tee -a $LOG_FILE
    grep "Failed password" /var/log/auth.log | tail -10 | tee -a $LOG_FILE
fi

# æ£€æŸ¥å¼‚å¸¸ç½‘ç»œè¿æ¥
echo "æ£€æŸ¥å¼‚å¸¸ç½‘ç»œè¿æ¥..." | tee -a $LOG_FILE
CONNECTIONS=$(ss -tn | wc -l)
if [ $CONNECTIONS -gt 1000 ]; then
    echo "è­¦å‘Š: æ£€æµ‹åˆ°å¼‚å¸¸ç½‘ç»œè¿æ¥æ•° ($CONNECTIONS)" | tee -a $LOG_FILE
fi

# æ£€æŸ¥æ–‡ä»¶å®Œæ•´æ€§
echo "æ£€æŸ¥å…³é”®æ–‡ä»¶å®Œæ•´æ€§..." | tee -a $LOG_FILE
CRITICAL_FILES=(
    "/etc/passwd"
    "/etc/shadow"
    "/etc/sudoers"
    "/opt/logistics-app/package.json"
)

for file in "${CRITICAL_FILES[@]}"; do
    if [ -f "$file" ]; then
        CURRENT_HASH=$(sha256sum "$file" | cut -d' ' -f1)
        STORED_HASH_FILE="/opt/security/hashes/$(basename $file).hash"
        
        if [ -f "$STORED_HASH_FILE" ]; then
            STORED_HASH=$(cat "$STORED_HASH_FILE")
            if [ "$CURRENT_HASH" != "$STORED_HASH" ]; then
                echo "è­¦å‘Š: æ–‡ä»¶ $file è¢«ä¿®æ”¹!" | tee -a $LOG_FILE
            fi
        else
            # é¦–æ¬¡è¿è¡Œï¼Œå­˜å‚¨å“ˆå¸Œå€¼
            mkdir -p /opt/security/hashes
            echo "$CURRENT_HASH" > "$STORED_HASH_FILE"
        fi
    fi
done

# æ£€æŸ¥è¿›ç¨‹å¼‚å¸¸
echo "æ£€æŸ¥è¿›ç¨‹å¼‚å¸¸..." | tee -a $LOG_FILE
SUSPICIOUS_PROCESSES=$(ps aux | grep -E "(nc|ncat|netcat|wget|curl)" | grep -v grep | wc -l)
if [ $SUSPICIOUS_PROCESSES -gt 0 ]; then
    echo "è­¦å‘Š: æ£€æµ‹åˆ°å¯ç–‘è¿›ç¨‹" | tee -a $LOG_FILE
    ps aux | grep -E "(nc|ncat|netcat|wget|curl)" | grep -v grep | tee -a $LOG_FILE
fi

echo "å…¥ä¾µæ£€æµ‹æ‰«æå®Œæˆ" | tee -a $LOG_FILE
```

#### 2. å®‰å…¨å®¡è®¡
```bash
#!/bin/bash
# security-audit.sh

REPORT_FILE="/tmp/security_audit_$(date +%Y%m%d_%H%M%S).txt"

echo "å¼€å§‹å®‰å…¨å®¡è®¡..." | tee $REPORT_FILE

# æ£€æŸ¥ç”¨æˆ·è´¦æˆ·
echo "=== ç”¨æˆ·è´¦æˆ·å®¡è®¡ ===" | tee -a $REPORT_FILE
echo "ç³»ç»Ÿç”¨æˆ·:" | tee -a $REPORT_FILE
cut -d: -f1 /etc/passwd | tee -a $REPORT_FILE

echo "å…·æœ‰sudoæƒé™çš„ç”¨æˆ·:" | tee -a $REPORT_FILE
grep -v '^#' /etc/sudoers | grep -v '^$' | tee -a $REPORT_FILE

# æ£€æŸ¥ç½‘ç»œæœåŠ¡
echo "=== ç½‘ç»œæœåŠ¡å®¡è®¡ ===" | tee -a $REPORT_FILE
echo "ç›‘å¬ç«¯å£:" | tee -a $REPORT_FILE
ss -tlnp | tee -a $REPORT_FILE

# æ£€æŸ¥å®šæ—¶ä»»åŠ¡
echo "=== å®šæ—¶ä»»åŠ¡å®¡è®¡ ===" | tee -a $REPORT_FILE
echo "ç³»ç»Ÿå®šæ—¶ä»»åŠ¡:" | tee -a $REPORT_FILE
ls -la /etc/cron.* | tee -a $REPORT_FILE

echo "ç”¨æˆ·å®šæ—¶ä»»åŠ¡:" | tee -a $REPORT_FILE
for user in $(cut -d: -f1 /etc/passwd); do
    crontab -u $user -l 2>/dev/null | grep -v '^#' | grep -v '^$' && echo "ç”¨æˆ·: $user" | tee -a $REPORT_FILE
done

# æ£€æŸ¥æ–‡ä»¶æƒé™
echo "=== æ–‡ä»¶æƒé™å®¡è®¡ ===" | tee -a $REPORT_FILE
echo "SUIDæ–‡ä»¶:" | tee -a $REPORT_FILE
find / -perm -4000 -type f 2>/dev/null | tee -a $REPORT_FILE

echo "ä¸–ç•Œå¯å†™æ–‡ä»¶:" | tee -a $REPORT_FILE
find / -perm -002 -type f 2>/dev/null | head -20 | tee -a $REPORT_FILE

# æ£€æŸ¥æ—¥å¿—
echo "=== æ—¥å¿—å®¡è®¡ ===" | tee -a $REPORT_FILE
echo "æœ€è¿‘çš„è®¤è¯å¤±è´¥:" | tee -a $REPORT_FILE
grep "authentication failure" /var/log/auth.log | tail -10 | tee -a $REPORT_FILE

echo "æœ€è¿‘çš„sudoä½¿ç”¨:" | tee -a $REPORT_FILE
grep "sudo:" /var/log/auth.log | tail -10 | tee -a $REPORT_FILE

echo "å®‰å…¨å®¡è®¡å®Œæˆï¼ŒæŠ¥å‘Šä¿å­˜åœ¨: $REPORT_FILE"
```

---

## ğŸ“š é™„å½•

### ğŸ”§ å¸¸ç”¨è¿ç»´å‘½ä»¤

#### 1. ç³»ç»Ÿç›‘æ§å‘½ä»¤
```bash
# å®æ—¶ç³»ç»Ÿç›‘æ§
htop

# ç£ç›˜ä½¿ç”¨æƒ…å†µ
df -h
du -sh /opt/logistics-app/*

# å†…å­˜ä½¿ç”¨è¯¦æƒ…
free -h
cat /proc/meminfo

# CPUä¿¡æ¯
lscpu
cat /proc/cpuinfo

# ç½‘ç»œè¿æ¥
ss -tuln
netstat -tlnp

# è¿›ç¨‹ç›‘æ§
ps aux --sort=-%cpu | head -10
ps aux --sort=-%mem | head -10

# æ—¥å¿—æŸ¥çœ‹
journalctl -f
tail -f /var/log/nginx/access.log
```

#### 2. Dockerç®¡ç†å‘½ä»¤
```bash
# æŸ¥çœ‹å®¹å™¨çŠ¶æ€
docker ps -a

# æŸ¥çœ‹æ—¥å¿—
docker logs -f logistics-app

# è¿›å…¥å®¹å™¨
docker exec -it logistics-app /bin/sh

# æ¸…ç†èµ„æº
docker system prune -f
docker volume prune -f

# æ›´æ–°æœåŠ¡
docker-compose pull
docker-compose up -d
```

#### 3. æ•°æ®åº“ç®¡ç†å‘½ä»¤
```bash
# è¿æ¥æ•°æ®åº“
psql $DATABASE_URL

# æŸ¥çœ‹è¿æ¥æ•°
SELECT count(*) FROM pg_stat_activity;

# æŸ¥çœ‹è¡¨å¤§å°
SELECT 
  schemaname,
  tablename,
  pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) as size
FROM pg_tables 
WHERE schemaname = 'public' 
ORDER BY pg_total_relation_size(schemaname||'.'||tablename) DESC;

# æŸ¥çœ‹æ…¢æŸ¥è¯¢
SELECT query, mean_exec_time, calls 
FROM pg_stat_statements 
WHERE mean_exec_time > 1000 
ORDER BY mean_exec_time DESC;
```

### ğŸ“ ç´§æ€¥è”ç³»æ–¹å¼

```
ç³»ç»Ÿç®¡ç†å‘˜: admin@company.com
æŠ€æœ¯è´Ÿè´£äºº: tech-lead@company.com
å®‰å…¨è´Ÿè´£äºº: security@company.com

24å°æ—¶å€¼ç­ç”µè¯: +86-xxx-xxxx-xxxx
ç´§æ€¥äº‹ä»¶å¤„ç†ç¾¤: ä¼ä¸šå¾®ä¿¡ç¾¤/é’‰é’‰ç¾¤
```

---

*æœ¬éƒ¨ç½²å’Œç»´æŠ¤æŒ‡å—æä¾›äº†ç‰©æµç®¡ç†ç³»ç»Ÿçš„å®Œæ•´è¿ç»´æ–¹æ¡ˆã€‚å»ºè®®è¿ç»´äººå‘˜å®šæœŸæ›´æ–°æ­¤æ–‡æ¡£ï¼Œç¡®ä¿ä¸å®é™…éƒ¨ç½²ç¯å¢ƒä¿æŒä¸€è‡´ã€‚*
